{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tonic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UiNvavTPevIL",
        "outputId": "83c9c1f8-c7f8-4af2-d7f8-4de1517d0f55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tonic\n",
            "  Downloading tonic-1.2.6-py3-none-any.whl (104 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.3/104.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pbr\n",
            "  Downloading pbr-5.11.1-py2.py3-none-any.whl (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.7/112.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from tonic) (1.10.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from tonic) (4.5.0)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (from tonic) (0.10.0.post2)\n",
            "Collecting expelliarmus\n",
            "  Downloading expelliarmus-1.1.12-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tonic) (1.22.4)\n",
            "Collecting importRosbag>=1.0.3\n",
            "  Downloading importRosbag-1.0.3.tar.gz (12 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from tonic) (4.65.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from tonic) (3.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from importRosbag>=1.0.3->tonic) (67.7.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa->tonic) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa->tonic) (0.56.4)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa->tonic) (0.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa->tonic) (0.12.1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa->tonic) (3.0.0)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa->tonic) (0.3.5)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa->tonic) (1.0.5)\n",
            "Requirement already satisfied: pooch<1.7,>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa->tonic) (1.6.0)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa->tonic) (1.2.0)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa->tonic) (1.2.2)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa->tonic) (0.39.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pooch<1.7,>=1.0->librosa->tonic) (23.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch<1.7,>=1.0->librosa->tonic) (2.27.1)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from pooch<1.7,>=1.0->librosa->tonic) (1.4.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa->tonic) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa->tonic) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->tonic) (2.21)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch<1.7,>=1.0->librosa->tonic) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch<1.7,>=1.0->librosa->tonic) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch<1.7,>=1.0->librosa->tonic) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch<1.7,>=1.0->librosa->tonic) (3.4)\n",
            "Building wheels for collected packages: importRosbag\n",
            "  Building wheel for importRosbag (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for importRosbag: filename=importRosbag-1.0.3-py3-none-any.whl size=25466 sha256=ea11726e8e1150dedfffdddc41be65155b8c1514bff67457e046ec53bc5d875b\n",
            "  Stored in directory: /root/.cache/pip/wheels/7f/a3/dd/959c5e661d227bac3fe3988df383132b18506f39831ffb2209\n",
            "Successfully built importRosbag\n",
            "Installing collected packages: pbr, importRosbag, expelliarmus, tonic\n",
            "Successfully installed expelliarmus-1.1.12 importRosbag-1.0.3 pbr-5.11.1 tonic-1.2.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tonic --pre"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1gF1cgde2_y",
        "outputId": "19286df8-958b-462f-b555-cba2ce489a7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tonic in /usr/local/lib/python3.10/dist-packages (1.2.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tonic) (1.22.4)\n",
            "Requirement already satisfied: importRosbag>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from tonic) (1.0.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from tonic) (4.65.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from tonic) (4.5.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from tonic) (3.8.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from tonic) (1.10.1)\n",
            "Requirement already satisfied: pbr in /usr/local/lib/python3.10/dist-packages (from tonic) (5.11.1)\n",
            "Requirement already satisfied: expelliarmus in /usr/local/lib/python3.10/dist-packages (from tonic) (1.1.12)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (from tonic) (0.10.0.post2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from importRosbag>=1.0.3->tonic) (67.7.2)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa->tonic) (0.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa->tonic) (0.3.5)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa->tonic) (1.2.0)\n",
            "Requirement already satisfied: pooch<1.7,>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa->tonic) (1.6.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa->tonic) (3.0.0)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa->tonic) (0.12.1)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa->tonic) (1.0.5)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa->tonic) (1.2.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa->tonic) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa->tonic) (0.56.4)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa->tonic) (0.39.1)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from pooch<1.7,>=1.0->librosa->tonic) (1.4.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch<1.7,>=1.0->librosa->tonic) (2.27.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pooch<1.7,>=1.0->librosa->tonic) (23.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa->tonic) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa->tonic) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->tonic) (2.21)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch<1.7,>=1.0->librosa->tonic) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch<1.7,>=1.0->librosa->tonic) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch<1.7,>=1.0->librosa->tonic) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch<1.7,>=1.0->librosa->tonic) (2022.12.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fHXBfk11mYN",
        "outputId": "ed21b156-e63a-43fa-9e1f-06dbf251f5b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.7/104.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.7/365.7 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.4/411.4 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.9/212.9 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
            "<ipython-input-3-94648b2f701f>:7: DeprecationWarning: The module snntorch.spikevision is deprecated. For loading neuromorphic datasets, we recommend using the Tonic project: https://github.com/neuromorphs/tonic\n",
            "  from snntorch.spikevision import spikedata\n"
          ]
        }
      ],
      "source": [
        "import urllib.request\n",
        "urllib.request.urlretrieve('https://raw.githubusercontent.com/jeshraghian/QSNNs/main/requirements.txt', 'requirements.txt')\n",
        "!pip install -r requirements.txt --quiet\n",
        "import torch, torch.nn as nn\n",
        "import snntorch as snn\n",
        "import brevitas.nn as qnn\n",
        "from snntorch.spikevision import spikedata\n",
        "\n",
        "\n",
        "def load_data(config):\n",
        "    data_dir = config[\"data_dir\"]\n",
        "    # Note: the train set / test set are of different durations, we used num_steps=100 due to memory limits.\n",
        "    # You will likely to improve our reported results by increasing num_steps=100 to 150.\n",
        "    trainset = spikedata.DVSGesture(data_dir, train=True, num_steps=100, dt=3000, ds=4)\n",
        "    testset = spikedata.DVSGesture(data_dir, train=False, num_steps=600, dt=3000, ds=4)\n",
        "    return trainset, testset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(config, net, trainloader, criterion, optimizer, device=\"cpu\", scheduler=None):\n",
        "    net.train()\n",
        "    loss_accum = []\n",
        "    lr_accum = []\n",
        "    i = 0\n",
        "    for data, labels in trainloader:\n",
        "        data, labels = data.to(device), labels.to(device)\n",
        "        spk_rec, _ = net(data.permute(1, 0, 2, 3, 4))\n",
        "        loss = criterion(spk_rec, labels.long())\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        if config[\"grad_clip\"]:\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), 1.0)\n",
        "\n",
        "        if config[\"weight_clip\"]:\n",
        "            with torch.no_grad():\n",
        "                for param in net.parameters():\n",
        "                    param.clamp_(-1, 1)\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        loss_accum.append(loss.item() / config[\"num_steps\"])\n",
        "        lr_accum.append(optimizer.param_groups[0][\"lr\"])\n",
        "\n",
        "    return loss_accum, lr_accum\n"
      ],
      "metadata": {
        "id": "cQauvvvV1qUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from snntorch import functional as SF\n",
        "\n",
        "\n",
        "def test(config, net, testloader, device=\"cpu\"):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        net.eval()\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs, _ = net(images.permute(1, 0, 2, 3, 4))\n",
        "            accuracy = SF.accuracy_rate(outputs, labels.long())\n",
        "            total += labels.size(0)\n",
        "            correct += accuracy * labels.size(0)\n",
        "\n",
        "    return 100 * correct / total\n"
      ],
      "metadata": {
        "id": "PMlKnCja2ZzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "class EarlyStopping_acc:\n",
        "    \"\"\"Early stops the training if test acc doesn't improve after a given patience.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, patience=7, verbose=False, delta=0, path=\"checkpoint.pt\", trace_func=print\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "            path (str): Path for the checkpoint to be saved to.\n",
        "                            Default: 'checkpoint.pt'\n",
        "            trace_func (function): trace print function.\n",
        "                            Default: print\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.test_loss_min = 0\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.trace_func = trace_func\n",
        "\n",
        "    def __call__(self, test_loss, model):\n",
        "        score = test_loss\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(test_loss, model)\n",
        "        elif score <= self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            self.trace_func(\n",
        "                f\"EarlyStopping counter: {self.counter} out of {self.patience}\"\n",
        "            )\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "                self.counter = 0\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(test_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, test_loss, model):\n",
        "        \"\"\"Saves model when test acc increases.\"\"\"\n",
        "        if self.verbose:\n",
        "            self.trace_func(\n",
        "                f\"Test acc increased ({self.test_loss_min:.6f} --> {test_loss:.6f}).  Saving model ...\"\n",
        "            )\n",
        "\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.test_loss_min = test_loss\n"
      ],
      "metadata": {
        "id": "4adF7nV72z96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def set_all_seeds(seed=0):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)"
      ],
      "metadata": {
        "id": "M2Q5BEF9201X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import snntorch as snn\n",
        "from snntorch import functional as SF\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "\n",
        "\n",
        "def evaluate(Net, config, load_data, train, test, optim_func):\n",
        "    file_name = config[\"exp_name\"]\n",
        "    for trial in range(config[\"num_trials_eval\"]):\n",
        "        csv_name = file_name + \"_t\" + str(trial) + \".csv\"\n",
        "        model_name = file_name + \"_t\" + str(trial) + \".pt\"\n",
        "        num_epochs = config[\"num_epochs_eval\"]\n",
        "        set_all_seeds(config[\"seed\"] + trial)\n",
        "        df_train_loss = pd.DataFrame()\n",
        "        df_test_acc = pd.DataFrame(columns=[\"epoch\", \"test_acc\", \"train_time\"])\n",
        "        df_lr = pd.DataFrame()\n",
        "        # Initialize the network\n",
        "        net = Net(config)\n",
        "        device = \"cpu\"\n",
        "        if torch.cuda.is_available():\n",
        "            device = \"cuda\"\n",
        "\n",
        "        net.to(device)\n",
        "        # Initialize the optimizer and scheduler\n",
        "        criterion = SF.mse_count_loss(\n",
        "            correct_rate=config[\"correct_rate\"], incorrect_rate=config[\"incorrect_rate\"]\n",
        "        )\n",
        "        optimizer, scheduler, loss_dependent = optim_func(net, config)\n",
        "        # Early stopping condition\n",
        "        if config[\"early_stopping\"]:\n",
        "            early_stopping = EarlyStopping_acc(\n",
        "                patience=config[\"patience\"], verbose=True, path=model_name\n",
        "            )\n",
        "            early_stopping.early_stop = False\n",
        "            early_stopping.best_score = None\n",
        "\n",
        "        # Load data\n",
        "        trainset, testset = load_data(config)\n",
        "        config[\"dataset_length\"] = len(trainset)\n",
        "        trainloader = DataLoader(\n",
        "            trainset, batch_size=int(config[\"batch_size\"]), shuffle=True\n",
        "        )\n",
        "        testloader = DataLoader(\n",
        "            testset, batch_size=int(config[\"batch_size\"]), shuffle=False\n",
        "        )\n",
        "        if loss_dependent:\n",
        "            old_loss_hist = float(\"inf\")\n",
        "\n",
        "        print(\n",
        "            f\"=======Trial: {trial}, Batch: {config['batch_size']}, beta: {config['beta']:.3f}, threshold: {config['threshold']:.2f}, slope: {config['slope']}, lr: {config['lr']:.3e}======\"\n",
        "        )\n",
        "        # Train\n",
        "        for epoch in range(num_epochs):\n",
        "            start_time = time.time()\n",
        "            loss_list, lr_list = train(\n",
        "                config, net, trainloader, criterion, optimizer, device, scheduler\n",
        "            )\n",
        "            epoch_time = time.time() - start_time\n",
        "            if loss_dependent:\n",
        "                avg_loss_hist = sum(loss_list) / len(loss_list)\n",
        "                if avg_loss_hist > old_loss_hist:\n",
        "                    for param_group in optimizer.param_groups:\n",
        "                        param_group[\"lr\"] = param_group[\"lr\"] * 0.5\n",
        "                else:\n",
        "                    old_loss_hist = avg_loss_hist\n",
        "\n",
        "            # Test\n",
        "            test_accuracy = test(config, net, testloader, device)\n",
        "            print(f\"Epoch: {epoch} \\tTest Accuracy: {test_accuracy}\")\n",
        "            df_lr = df_lr.append(lr_list, ignore_index=True)\n",
        "\n",
        "            df_train_loss = df_train_loss.append(loss_list, ignore_index=True)\n",
        "            df_test_acc = df_test_acc.append(\n",
        "                {\"epoch\": epoch, \"test_acc\": test_accuracy, \"train_time\": epoch_time},\n",
        "                ignore_index=True,\n",
        "            )\n",
        "            if config[\"save_csv\"]:\n",
        "                df_train_loss.to_csv(\"loss_\" + csv_name, index=False)\n",
        "                df_test_acc.to_csv(\"acc_\" + csv_name, index=False)\n",
        "                df_lr.to_csv(\"lr_\" + csv_name, index=False)\n",
        "\n",
        "            if config[\"early_stopping\"]:\n",
        "                early_stopping(test_accuracy, net)\n",
        "                if early_stopping.early_stop:\n",
        "                    print(\"Early stopping\")\n",
        "                    early_stopping.early_stop = False\n",
        "                    early_stopping.best_score = None\n",
        "                    break\n"
      ],
      "metadata": {
        "id": "A_TxUu2Z2oi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import snntorch as snn\n",
        "from snntorch import surrogate\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import brevitas.nn as qnn\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.num_bits = config[\"num_bits\"]\n",
        "        self.thr = config[\"threshold\"]\n",
        "        self.slope = config[\"slope\"]\n",
        "        self.beta = config[\"beta\"]\n",
        "        self.num_steps = config[\"num_steps\"]\n",
        "        self.batch_norm = config[\"batch_norm\"]\n",
        "        self.p1 = config[\"dropout\"]\n",
        "        self.spike_grad = surrogate.fast_sigmoid(self.slope)\n",
        "        if self.num_bits is None:\n",
        "            self.init_net()\n",
        "        else:\n",
        "            self.init_quantized_net()\n",
        "\n",
        "    def init_net(self):\n",
        "        self.conv1 = nn.Conv2d(2, 16, 5, bias=False)\n",
        "        self.conv1_bn = nn.BatchNorm2d(16)\n",
        "        self.lif1 = snn.Leaky(self.beta, threshold=self.thr, spike_grad=self.spike_grad)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 5, bias=False)\n",
        "        self.conv2_bn = nn.BatchNorm2d(32)\n",
        "        self.lif2 = snn.Leaky(self.beta, threshold=self.thr, spike_grad=self.spike_grad)\n",
        "        self.fc1 = nn.Linear(32 * 5 * 5, 11)\n",
        "        self.lif3 = snn.Leaky(self.beta, threshold=self.thr, spike_grad=self.spike_grad)\n",
        "        self.dropout = nn.Dropout(self.p1)\n",
        "\n",
        "    def init_quantized_net(self):\n",
        "        self.conv1 = qnn.QuantConv2d(\n",
        "            2, 16, 5, bias=False, weight_bit_width=self.num_bits\n",
        "        )\n",
        "        self.conv1_bn = nn.BatchNorm2d(16)\n",
        "        self.lif1 = snn.Leaky(self.beta, threshold=self.thr, spike_grad=self.spike_grad)\n",
        "        self.conv2 = qnn.QuantConv2d(\n",
        "            16, 32, 5, bias=False, weight_bit_width=self.num_bits\n",
        "        )\n",
        "        self.conv2_bn = nn.BatchNorm2d(32)\n",
        "        self.lif2 = snn.Leaky(self.beta, threshold=self.thr, spike_grad=self.spike_grad)\n",
        "        self.fc1 = qnn.QuantLinear(\n",
        "            32 * 5 * 5, 11, bias=False, weight_bit_width=self.num_bits\n",
        "        )\n",
        "        self.lif3 = snn.Leaky(self.beta, threshold=self.thr, spike_grad=self.spike_grad)\n",
        "        self.dropout = nn.Dropout(self.p1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initialize hidden states and outputs at t=0\n",
        "        mem1 = self.lif1.init_leaky()\n",
        "        mem2 = self.lif2.init_leaky()\n",
        "        mem3 = self.lif3.init_leaky()\n",
        "        # Record the final layer\n",
        "        spk3_rec = []\n",
        "        mem3_rec = []\n",
        "        for step in range(x.size(0)):\n",
        "            cur1 = F.avg_pool2d(self.conv1(x[step]), 2)\n",
        "            if self.batch_norm:\n",
        "                cur1 = self.conv1_bn(cur1)\n",
        "\n",
        "            spk1, mem1 = self.lif1(cur1, mem1)\n",
        "            cur2 = F.avg_pool2d(self.conv2(spk1), 2)\n",
        "            if self.batch_norm:\n",
        "                cur2 = self.conv2_bn(cur2)\n",
        "\n",
        "            spk2, mem2 = self.lif2(cur2, mem2)\n",
        "            cur3 = self.dropout(self.fc1(spk2.flatten(1)))\n",
        "            spk3, mem3 = self.lif3(cur3, mem3)\n",
        "            spk3_rec.append(spk3)\n",
        "            mem3_rec.append(mem3)\n",
        "\n",
        "        return torch.stack(spk3_rec, dim=0), torch.stack(mem3_rec, dim=0)\n"
      ],
      "metadata": {
        "id": "jXqWEwHN3L1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import sys\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "config = {\n",
        "    \"exp_name\": \"DVS\",  # Experiment name\n",
        "    \"num_trials_eval\": 3,  # Number of trails to execute (separate training and evaluation instances)\n",
        "    \"num_epochs_eval\": 500,  # Number of epochs to train for (per trial)\n",
        "    \"data_dir\": \"~/data/\",  # Data directory to download and store data\n",
        "    \"batch_size\": 16,  # Batch size\n",
        "    \"seed\": 0,  # Random seed\n",
        "    \"num_workers\": 0,  # Number of workers for the dataloader\n",
        "    \"num_bits\": 4,  # Bit resolution. If None, floating point resolution is used\n",
        "    \"save_csv\": True,  # Whether or not to save loss, lr, and accuracy dataframes\n",
        "    \"early_stopping\": True,  # Whether or not to use early stopping\n",
        "    \"patience\": 100,  # Number of epochs to wait for improvement before stopping\n",
        "    # Network parameters\n",
        "    \"grad_clip\": True,  # Whether or not to clip gradients\n",
        "    \"weight_clip\": True,  # Whether or not to clip weights\n",
        "    \"batch_norm\": False,  # Whether or not to use batch normalization\n",
        "    \"dropout\": 0.203,  # Dropout rate\n",
        "    \"beta\": 0.614,  # Decay rate parameter (beta)\n",
        "    \"threshold\": 0.427,  # Threshold parameter (theta)\n",
        "    \"lr\": 2.634e-3,  # Initial learning rate\n",
        "    \"slope\": 4.413,  # Slope value (k)\n",
        "    # Fixed params\n",
        "    \"num_steps\": 1,  # Number of timesteps to encode input for 100 TODO\n",
        "    \"correct_rate\": 0.8,  # Correct rate\n",
        "    \"incorrect_rate\": 0.2,  # Incorrect rate\n",
        "    \"betas\": (0.9, 0.999),  # Adam optimizer beta values\n",
        "    \"t_max\": 735,  # Frequency of the cosine annealing scheduler (5 epochs)\n",
        "    \"t_0\": 735,  # Initial frequency of the cosine annealing scheduler\n",
        "    \"t_mult\": 2,  # The frequency of cosine is halved after every 4690 iters (10 epochs)\n",
        "    \"eta_min\": 0,  # Minimum learning rate\n",
        "}\n",
        "\n",
        "\n",
        "def optim_func(net, config):\n",
        "    optimizer = torch.optim.Adam(\n",
        "        net.parameters(), lr=config[\"lr\"], betas=config[\"betas\"]\n",
        "    )\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer, T_max=config[\"t_0\"], eta_min=config[\"eta_min\"], last_epoch=-1\n",
        "    )\n",
        "    loss_dependent = False\n",
        "    return optimizer, scheduler, loss_dependent\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    evaluate(Net, config, load_data, train, test, optim_func)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "id": "5L3k2yCi2fSS",
        "outputId": "a290128f-dd54-4d62-c5c0-feb89240899c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-e10c8c059be6>\u001b[0m in \u001b[0;36m<cell line: 53>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-5932a21b88d9>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(Net, config, load_data, train, test, optim_func)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# Load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mtrainset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dataset_length\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         trainloader = DataLoader(\n",
            "\u001b[0;32m<ipython-input-3-94648b2f701f>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Note: the train set / test set are of different durations, we used num_steps=100 due to memory limits.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# You will likely to improve our reported results by increasing num_steps=100 to 150.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mtrainset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspikedata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDVSGesture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mtestset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspikedata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDVSGesture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrainset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/snntorch/spikevision/spikedata/dvs_gesture.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, train, transform, target_transform, download_and_create, num_steps, dt, ds, return_meta, time_shuffle)\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0mtarget_transform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCompose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoOneHot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         super(DVSGesture, self).__init__(\n\u001b[0m\u001b[1;32m    235\u001b[0m             \u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mroot\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhdf5_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/snntorch/spikevision/neuromorphic_dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transforms, transform, target_transform, transform_train, transform_test, target_transform_train, target_transform_test)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mtarget_transform_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     ):\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m             \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute '_six'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_3SL_TFc3Ohk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}